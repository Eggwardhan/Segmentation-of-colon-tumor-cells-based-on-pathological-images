{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-clone",
   "metadata": {},
   "source": [
    "## UNet with ResNet34 encoder (Pytorch)\n",
    "<code>https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch/notebook\\<code/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "train_mask_dir = \"../DigestPath2019/train_mask\" #create  train mask\n",
    "train_dir = \"../DigestPath2019/train\"\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    # BasicBlock and BottleNeck block\n",
    "    # have different output size\n",
    "    # we use class attribute expansion\n",
    "    # to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        # shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # the shortcut output dimension is not the same with residual function\n",
    "        # use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channel,out_channel, block, num_block):\n",
    "        super().__init__()\n",
    "        #self.in_channels = in_channel\n",
    "        self.outc = out_channel #不能加self.out_channel  不是很懂为啥 估计是变量优先级问题\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size = 7, stride = 2, padding = 3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.dconv_last=nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64,out_channel,1)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)# [stride, 1,1,1...]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        conv1 = self.conv1(x)\n",
    "        temp=self.maxpool(conv1)\n",
    "        print(temp.shape)\n",
    "        print(self.conv2_x)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        x = self.upsample(bottle)\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv2.shape)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x=self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv1.shape)\n",
    "        x=torch.cat([x,conv1],dim=1)\n",
    "        out=self.dconv_last(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "         # 导入自己模型的参数\n",
    "        model_dict=self.state_dict()\n",
    "        \n",
    "        resnet34_weights = models.resnet34(True).state_dict()\n",
    "        count_res = 0\n",
    "        count_my = 0\n",
    "\n",
    "        reskeys = list(resnet34_weights.keys())\n",
    "        mykeys = list(model_dict.keys())\n",
    "        # print(self)  自己网络的结构\n",
    "        # print(models.resnet34())   resnet结构\n",
    "        # print(reskeys)\n",
    "        # print(mykeys)\n",
    "\n",
    "        corresp_map = []\n",
    "        while (True):              # 后缀相同的放入list\n",
    "            reskey = reskeys[count_res]\n",
    "            mykey = mykeys[count_my]\n",
    "\n",
    "            if \"fc\" in reskey:\n",
    "                break\n",
    "\n",
    "            while reskey.split(\".\")[-1] not in mykey:\n",
    "                count_my += 1\n",
    "                mykey = mykeys[count_my]\n",
    "\n",
    "            corresp_map.append([reskey, mykey])\n",
    "            count_res += 1\n",
    "            count_my += 1\n",
    "\n",
    "        for k_res, k_my in corresp_map:\n",
    "            model_dict[k_my]=resnet34_weights[k_res]\n",
    "\n",
    "        try:\n",
    "            self.load_state_dict(model_dict)\n",
    "            print(\"Loaded resnet34 weights in mynet !\")\n",
    "        except:\n",
    "            print(\"Error resnet34 weights in mynet !\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet34(in_channel,out_channel,pretrain=True):\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    model=ResNet(in_channel,out_channel,BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrain:\n",
    "        model.load_pretrained_weights()\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet34(3, 4, False) # out_channel = 4  4分类问题\n",
    "    #print(net)\n",
    "    x = torch.rand((1, 3, 512, 512)) #N，C, H, W\n",
    "    print(net.forward(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, preprocess=None,scale=0.1, mask_suffix='_mask'):      \n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.preprocess= preprocess\n",
    "        #self.scale = scale\n",
    "        self.scale = 512\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]   # get prefix or so-called id\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "        #print(\"image_directory:{}  with {} files.\\nmask_dir:{}\".format(imgs_dir,len(self.ids),masks_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def process(cls, pil_img):\n",
    "        #w, h = pil_img.size\n",
    "        img = np.array(pil_img)\n",
    "        # unnormalize\n",
    "        opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        #print(\"cv2 show by process\")\n",
    "        #cv2.imshow(\"sss\",opencvImage)\n",
    "        \n",
    "        return opencvImage\n",
    "        #newW, newH = int(scale * w), int(scale * h)\n",
    "        #assert w > 0 and h > 0, 'Scale is too small\n",
    "        #pil_img = pil_img.resize((size, size))\n",
    "\n",
    "        ''' img_nd = np.array(pil_img)\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW\n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans'''\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        temp = os.path.join(self.masks_dir,idx+self.mask_suffix+'.*')\n",
    "        mask_file = glob.glob(temp)\n",
    "\n",
    "        img_file = glob.glob(os.path.join(self.imgs_dir , idx + '.*'))\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        #print(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "        img = self.process(img)\n",
    "        mask = self.process(mask)\n",
    "        if self.preprocess!= None:\n",
    "            transformed = self.preprocess(image=img,mask=mask)\n",
    "            img= transformed['image']\n",
    "            mask= transformed['mask']\n",
    "        img_trans = img.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "        #print(mask.shape)\n",
    "        Grayimg = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask_trans = cv2.threshold(Grayimg, 12, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "        \n",
    "        _ = {\n",
    "            'image': torch.from_numpy(img_trans).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask_trans).type(torch.FloatTensor).unsqueeze(0)\n",
    "        }      \n",
    "        print(\"image final .shape\",_['image'].shape)\n",
    "\n",
    "        print(\"mask final .shape\",_['mask'].shape)\n",
    "        #show_img(img)\n",
    "        return _\n"
   ]
  },
  {
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n",
    "    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n",
    "    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n",
    "    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n",
    ")\n",
    "size = 512\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    OneOf([\n",
    "        RandomContrast(),\n",
    "        RandomGamma(),\n",
    "        RandomBrightness()\n",
    "    ], p=0.3),\n",
    "    OneOf([\n",
    "        ElasticTransform(alpha = 120, sigma=120*0.05,alpha_affine = 12*0.03),\n",
    "        GridDistortion(),\n",
    "        OpticalDistortion(distort_limit = 2, shift_limit = 0.5)\n",
    "    ], p=0.3),\n",
    "    RandomSizedCrop(min_max_height=(512,1024),height = size, width =size,p=1),\n",
    "    ToFloat(max_value=1)\n",
    "], p =1)\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    ToFloat(max_value=1)\n",
    "],p=1)\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img.astype(int))\n",
    "    plt.show()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url=\"../Colonoscopy_tissue_segment_dataset/train/1902160001_2019-06-11 12_36_32-lv1-39045-16016-3312-4096.jpg\"\n",
    "image = Image.open(img_url)\n",
    "img = np.array(image)\n",
    "\n",
    "print(img.size)\n",
    "\n",
    "print(image.size)\n",
    "\n",
    "opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# unnormalize\n",
    "print(\"cv2 show by process\")\n",
    "#cv2.imshow(\"sss\",opencvImage)\n",
    "opencvImage=AUGMENTATIONS_TRAIN(image=opencvImage)['image']\n",
    "print(opencvImage.shape)\n",
    "img = opencvImage.transpose((2, 0, 1))\n",
    "if img.max() > 1:\n",
    "    img = img / 255 \n",
    "print(img.shape)\n",
    "\n",
    "tmp = torch.from_numpy(img).type(torch.FloatTensor)\n",
    "\n",
    "print(\"tmp\",tmp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 5736, 3538])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 256, 3538] instead",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c4e95456ef6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtotal_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;31m#print(type(pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-c4e95456ef6c>\u001b[0m in \u001b[0;36mtotal_predict\u001b[0;34m(ori_image)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/model/nested_unet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mx0_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mx1_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx0_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/model/nested_unet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 256, 3538] instead"
     ]
    }
   ],
   "source": [
    "import predict \n",
    "from train import BasicDataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import cv2\n",
    "device = \"cpu\"\n",
    "img_url = \"/Users/eggwardhan/Documents/cv/Colonoscopy_tissue_segment_dataset/val/18_00991B_2019-05-07 21_27_54-lv1-16174-30030-3538-5736.jpg\"\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    ToFloat(max_value=1)\n",
    "],p=1)\n",
    "load=\"/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth\"\n",
    "net = model.choose_net(\"nested_unet\")\n",
    "\n",
    "net = net(in_channel=3,out_channel=1)\n",
    "net.load_state_dict(\n",
    "            torch.load(load, map_location=device)\n",
    "            )\n",
    "\n",
    "def total_predict(ori_image):\n",
    "    ori_image = BasicDataset.process2(pil_img=ori_image)['image']\n",
    "    \n",
    "    print(ori_image.shape)\n",
    "    h_step = ori_image.shape[1]\n",
    "    w_step = ori_image.shape[2]\n",
    "    \n",
    "    h_rest = -(ori_image.shape[1] - 256 * h_step)\n",
    "    w_rest = -(ori_image.shape[2] - 256 * w_step)\n",
    "    image_list = []\n",
    "    predict_img = []\n",
    "    for h in range(h_step):\n",
    "        for w in range(w_step):\n",
    "            image_sample = ori_image[(h*256):(h*256+256),\n",
    "            (w*256 ):(w*256 + 256), :]\n",
    "            image_list.append(image_sample)\n",
    "        image_list.append(ori_image[( h* 256):(h*256 +256), -256:, :])\n",
    "    for w in range(w_step-1):\n",
    "        image_list.append(ori_image[-256:, (w*256):(w*256 +256), :])\n",
    "    image_list.append(ori_image[-256:, -256:, :])\n",
    "    \n",
    "    \n",
    "    for image in image_list:\n",
    "        pred1 = net(image)\n",
    "        pred1 = pred1.squeeze(0).astype(np.int8)\n",
    "        break\n",
    "    prinrt(pred1)\n",
    "    return pred1\n",
    "\n",
    "pred= total_predict(Image.open(img_url))\n",
    "#print(type(pred))\n",
    "#print(pred)\n",
    "show_img(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[0.7855, 0.8603, 0.8442,  ..., 0.7321, 0.7203, 0.6647],\n          [0.8496, 0.9220, 0.8959,  ..., 0.7582, 0.7735, 0.7122],\n          [0.8292, 0.8919, 0.8305,  ..., 0.6330, 0.6918, 0.6656],\n          ...,\n          [0.5537, 0.5200, 0.4550,  ..., 0.8746, 0.8544, 0.7611],\n          [0.5540, 0.5326, 0.4862,  ..., 0.8267, 0.7935, 0.7023],\n          [0.5573, 0.5528, 0.5514,  ..., 0.7205, 0.6864, 0.6434]]]],\n       grad_fn=<SigmoidBackward>)\n(256, 256)\n<class 'numpy.ndarray'>\n[[ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]\n ...\n [ True  True False ...  True  True  True]\n [ True  True False ...  True  True  True]\n [ True  True  True ...  True  True  True]]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1296x1080 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"848.684766pt\" version=\"1.1\" viewBox=\"0 0 855.8875 848.684766\" width=\"855.8875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-19T11:29:30.643972</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 848.684766 \nL 855.8875 848.684766 \nL 855.8875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 824.806641 \nL 848.6875 824.806641 \nL 848.6875 9.406641 \nL 33.2875 9.406641 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p47027f8c45)\">\n    <image height=\"816\" id=\"imageab834f758d\" transform=\"scale(1 -1)translate(0 -816)\" width=\"816\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-8.806641\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc52995c21e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.880078\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(31.698828 839.405078)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.137891\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(187.775391 839.405078)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"353.395703\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(343.851953 839.405078)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"512.653516\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(503.109766 839.405078)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"671.911328\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(662.367578 839.405078)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"831.169141\" xlink:href=\"#mc52995c21e\" y=\"824.806641\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(821.625391 839.405078)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0fe8a1aa4b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"170.257031\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 174.05625)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"329.514844\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 333.314062)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"488.772656\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 492.571875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"648.030469\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 651.829688)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0fe8a1aa4b\" y=\"807.288281\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 811.0875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 824.806641 \nL 33.2875 9.406641 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 848.6875 824.806641 \nL 848.6875 9.406641 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 824.806641 \nL 848.6875 824.806641 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 9.406641 \nL 848.6875 9.406641 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p47027f8c45\">\n   <rect height=\"815.4\" width=\"815.4\" x=\"33.2875\" y=\"9.406641\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# ########   predict test    #########\n",
    "import predict \n",
    "from train import BasicDataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import cv2\n",
    "device = \"cpu\"\n",
    "img_url = \"/Users/eggwardhan/Documents/cv/Colonoscopy_tissue_segment_dataset/val/18_00991B_2019-05-07 21_27_54-lv1-16174-30030-3538-5736.jpg\"\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    ToFloat(max_value=1)\n",
    "],p=1)\n",
    "load=\"/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth\"\n",
    "net = model.choose_net(\"nested_unet\")\n",
    "net = net(in_channel=3,out_channel=1)\n",
    "net.load_state_dict(\n",
    "            torch.load(load, map_location=device)\n",
    "            )\n",
    "def total_predict(ori_image):\n",
    "    ori_image = np.array(ori_image)\n",
    "    #print(img.shape)\n",
    "    h_step = ori_image.shape[0]//256\n",
    "    w_step = ori_image.shape[1]//256\n",
    "    \n",
    "    h_rest = -(ori_image.shape[0] - 256 * h_step)\n",
    "    w_rest = -(ori_image.shape[1] - 256 * w_step)\n",
    "    image_list = []\n",
    "    predict_img = []\n",
    "    h =w=8\n",
    "    image_sample = ori_image[(h*256):(h*256+256),\n",
    "    (w*256 ):(w*256 + 256), :]\n",
    "    image_list.append(image_sample)\n",
    "    image_list.append(ori_image[( h* 256):(h*256 +256), -256:, :])\n",
    "    \n",
    "    for image in image_list:\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        image=image.unsqueeze(0)\n",
    "        \n",
    "        pred1 = torch.sigmoid(net(image))\n",
    "        break\n",
    "    print(pred1)\n",
    "    return pred1>0.5\n",
    "\n",
    "pred= total_predict(Image.open(img_url))\n",
    "\n",
    "pred = pred.squeeze().detach().numpy()\n",
    "\n",
    "print(pred.shape)\n",
    "print(type(pred))\n",
    "print(pred)\n",
    "\n",
    "\n",
    "def show_img(img):\n",
    "\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "#show_img(pred.convert(\"L\"))\n",
    "#print(Image.fromarray(pred).convert('1'))\n",
    "show_img(Image.fromarray(pred) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'JpegImageFile' and 'int'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e0d19e46085a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'testimage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"___________________________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/train.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmask_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_trans\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         '''\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;31m#print(\"mask shape\",_['mask'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m        \u001b[0;31m# print(\"mask tensor\",_['mask'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'JpegImageFile' and 'int'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import model.model as model\n",
    "from augment import AUGMENTATIONS_TRAIN,AUGMENTATIONS_TEST\n",
    "from train import BasicDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_mask_dir = \"../Colonoscopy_tissue_segment_dataset/train_mask\" #create  train mask\n",
    "train_dir = \"../Colonoscopy_tissue_segment_dataset/train\" # create train data\n",
    "val_mask_dir = \"../Colonoscopy_tissue_segment_dataset/val_mask\"\n",
    "val_dir = \"../Colonoscopy_tissue_segment_dataset/val\"\n",
    "test_mask_dir=\"../Colonoscopy_tissue_segment_dataset/test_mask\"\n",
    "\n",
    "\n",
    "net = model.choose_net(\"nested_unet\")\n",
    "net = net(in_channel=3,out_channel=1)\n",
    "dataset2 = BasicDataset(test_mask_dir,test_mask_dir,AUGMENTATIONS_TEST)\n",
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    if type(img)==torch.Tensor :\n",
    "        img = img.squeeze(0)\n",
    "\n",
    "        img = img.detach().numpy()\n",
    "        img = img.transpose((1,2, 0))\n",
    "        print(\"transposed image shape\",img.shape)\n",
    "        img=img/255\n",
    "    print(\"img type when show :\",type(img))\n",
    "    print(\"_________show_img________\")\n",
    "    print(img)\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    img = img.astype(int)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "n_val = len(dataset2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "val_loader = DataLoader(dataset2, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "writer = SummaryWriter(comment=f'testimage')\n",
    "for batch in val_loader:\n",
    "    image = batch['mask']\n",
    "    print(\"___________________________\")\n",
    "    image.to(device=device, dtype=torch.float32)\n",
    "    writer.add_images('masks/pred', torch.sigmoid(image) > 0.5, 0)\n",
    "    writer.add_images('masks/^255', torch.sigmoid(image/255) > 0.5, 0)\n",
    "    #print(\"\"image)\n",
    "    show_img(image)\n",
    "    break\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "#val_score = eval_net(net, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}