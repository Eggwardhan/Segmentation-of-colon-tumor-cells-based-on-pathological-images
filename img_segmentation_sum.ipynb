{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-clone",
   "metadata": {},
   "source": [
    "## UNet with ResNet34 encoder (Pytorch)\n",
    "<code>https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch/notebook\\<code/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "train_mask_dir = \"../DigestPath2019/train_mask\" #create  train mask\n",
    "train_dir = \"../DigestPath2019/train\"\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    # BasicBlock and BottleNeck block\n",
    "    # have different output size\n",
    "    # we use class attribute expansion\n",
    "    # to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        # shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # the shortcut output dimension is not the same with residual function\n",
    "        # use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channel,out_channel, block, num_block):\n",
    "        super().__init__()\n",
    "        #self.in_channels = in_channel\n",
    "        self.outc = out_channel #不能加self.out_channel  不是很懂为啥 估计是变量优先级问题\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size = 7, stride = 2, padding = 3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.dconv_last=nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64,out_channel,1)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)# [stride, 1,1,1...]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        conv1 = self.conv1(x)\n",
    "        temp=self.maxpool(conv1)\n",
    "        print(temp.shape)\n",
    "        print(self.conv2_x)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        x = self.upsample(bottle)\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv2.shape)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x=self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv1.shape)\n",
    "        x=torch.cat([x,conv1],dim=1)\n",
    "        out=self.dconv_last(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "         # 导入自己模型的参数\n",
    "        model_dict=self.state_dict()\n",
    "        \n",
    "        resnet34_weights = models.resnet34(True).state_dict()\n",
    "        count_res = 0\n",
    "        count_my = 0\n",
    "\n",
    "        reskeys = list(resnet34_weights.keys())\n",
    "        mykeys = list(model_dict.keys())\n",
    "        # print(self)  自己网络的结构\n",
    "        # print(models.resnet34())   resnet结构\n",
    "        # print(reskeys)\n",
    "        # print(mykeys)\n",
    "\n",
    "        corresp_map = []\n",
    "        while (True):              # 后缀相同的放入list\n",
    "            reskey = reskeys[count_res]\n",
    "            mykey = mykeys[count_my]\n",
    "\n",
    "            if \"fc\" in reskey:\n",
    "                break\n",
    "\n",
    "            while reskey.split(\".\")[-1] not in mykey:\n",
    "                count_my += 1\n",
    "                mykey = mykeys[count_my]\n",
    "\n",
    "            corresp_map.append([reskey, mykey])\n",
    "            count_res += 1\n",
    "            count_my += 1\n",
    "\n",
    "        for k_res, k_my in corresp_map:\n",
    "            model_dict[k_my]=resnet34_weights[k_res]\n",
    "\n",
    "        try:\n",
    "            self.load_state_dict(model_dict)\n",
    "            print(\"Loaded resnet34 weights in mynet !\")\n",
    "        except:\n",
    "            print(\"Error resnet34 weights in mynet !\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet34(in_channel,out_channel,pretrain=True):\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    model=ResNet(in_channel,out_channel,BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrain:\n",
    "        model.load_pretrained_weights()\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet34(3, 4, False) # out_channel = 4  4分类问题\n",
    "    #print(net)\n",
    "    x = torch.rand((1, 3, 512, 512)) #N，C, H, W\n",
    "    print(net.forward(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, preprocess=None,scale=0.1, mask_suffix='_mask'):      \n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.preprocess= preprocess\n",
    "        #self.scale = scale\n",
    "        self.scale = 512\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]   # get prefix or so-called id\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "        #print(\"image_directory:{}  with {} files.\\nmask_dir:{}\".format(imgs_dir,len(self.ids),masks_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def process(cls, pil_img):\n",
    "        #w, h = pil_img.size\n",
    "        img = np.array(pil_img)\n",
    "        # unnormalize\n",
    "        opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        #print(\"cv2 show by process\")\n",
    "        #cv2.imshow(\"sss\",opencvImage)\n",
    "        \n",
    "        return opencvImage\n",
    "        #newW, newH = int(scale * w), int(scale * h)\n",
    "        #assert w > 0 and h > 0, 'Scale is too small\n",
    "        #pil_img = pil_img.resize((size, size))\n",
    "\n",
    "        ''' img_nd = np.array(pil_img)\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW\n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans'''\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        temp = os.path.join(self.masks_dir,idx+self.mask_suffix+'.*')\n",
    "        mask_file = glob.glob(temp)\n",
    "\n",
    "        img_file = glob.glob(os.path.join(self.imgs_dir , idx + '.*'))\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        #print(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "        img = self.process(img)\n",
    "        mask = self.process(mask)\n",
    "        if self.preprocess!= None:\n",
    "            transformed = self.preprocess(image=img,mask=mask)\n",
    "            img= transformed['image']\n",
    "            mask= transformed['mask']\n",
    "        img_trans = img.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "        #print(mask.shape)\n",
    "        Grayimg = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask_trans = cv2.threshold(Grayimg, 12, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "        \n",
    "        _ = {\n",
    "            'image': torch.from_numpy(img_trans).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask_trans).type(torch.FloatTensor).unsqueeze(0)\n",
    "        }      \n",
    "        print(\"image final .shape\",_['image'].shape)\n",
    "\n",
    "        print(\"mask final .shape\",_['mask'].shape)\n",
    "        #show_img(img)\n",
    "        return _\n"
   ]
  },
  {
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n",
    "    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n",
    "    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n",
    "    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n",
    ")\n",
    "size = 512\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    OneOf([\n",
    "        RandomContrast(),\n",
    "        RandomGamma(),\n",
    "        RandomBrightness()\n",
    "    ], p=0.3),\n",
    "    OneOf([\n",
    "        ElasticTransform(alpha = 120, sigma=120*0.05,alpha_affine = 12*0.03),\n",
    "        GridDistortion(),\n",
    "        OpticalDistortion(distort_limit = 2, shift_limit = 0.5)\n",
    "    ], p=0.3),\n",
    "    RandomSizedCrop(min_max_height=(512,1024),height = size, width =size,p=1),\n",
    "    ToFloat(max_value=1)\n",
    "], p =1)\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    ToFloat(max_value=1)\n",
    "],p=1)\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img.astype(int))\n",
    "    plt.show()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url=\"../Colonoscopy_tissue_segment_dataset/train/1902160001_2019-06-11 12_36_32-lv1-39045-16016-3312-4096.jpg\"\n",
    "image = Image.open(img_url)\n",
    "img = np.array(image)\n",
    "\n",
    "print(img.size)\n",
    "\n",
    "print(image.size)\n",
    "\n",
    "opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# unnormalize\n",
    "print(\"cv2 show by process\")\n",
    "#cv2.imshow(\"sss\",opencvImage)\n",
    "opencvImage=AUGMENTATIONS_TRAIN(image=opencvImage)['image']\n",
    "print(opencvImage.shape)\n",
    "img = opencvImage.transpose((2, 0, 1))\n",
    "if img.max() > 1:\n",
    "    img = img / 255 \n",
    "print(img.shape)\n",
    "\n",
    "tmp = torch.from_numpy(img).type(torch.FloatTensor)\n",
    "\n",
    "print(\"tmp\",tmp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-06bcdf06bc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m net.load_state_dict(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth'"
     ]
    }
   ],
   "source": [
    "import predict \n",
    "from train import BasicDataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "from augment import AUGMENTATIONS_TEST2 as AUGMENTATIONS_TEST\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import cv2\n",
    "device = \"cpu\"\n",
    "img_url = \"/Users/eggwardhan/Documents/cv/Colonoscopy_tissue_segment_dataset/val/18_00991B_2019-05-07 21_27_54-lv1-16174-30030-3538-5736.jpg\"\n",
    "\n",
    "load=\"/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth\"\n",
    "net = model.choose_net(\"nested_unet\")\n",
    "\n",
    "net = net(in_channel=3,out_channel=1)\n",
    "net.load_state_dict(\n",
    "            torch.load(load, map_location=device)\n",
    "            )\n",
    "\n",
    "def total_predict(ori_image):\n",
    "    ori_image = BasicDataset.process2(pil_img=ori_image)['image']\n",
    "    \n",
    "    print(ori_image.shape)\n",
    "    h_step = ori_image.shape[1]\n",
    "    w_step = ori_image.shape[2]\n",
    "    \n",
    "    h_rest = -(ori_image.shape[1] - 256 * h_step)\n",
    "    w_rest = -(ori_image.shape[2] - 256 * w_step)\n",
    "    image_list = []\n",
    "    predict_img = []\n",
    "    for h in range(h_step):\n",
    "        for w in range(w_step):\n",
    "            image_sample = ori_image[(h*256):(h*256+256),\n",
    "            (w*256 ):(w*256 + 256), :]\n",
    "            image_list.append(image_sample)\n",
    "        image_list.append(ori_image[( h* 256):(h*256 +256), -256:, :])\n",
    "    for w in range(w_step-1):\n",
    "        image_list.append(ori_image[-256:, (w*256):(w*256 +256), :])\n",
    "    image_list.append(ori_image[-256:, -256:, :])\n",
    "    \n",
    "    \n",
    "    for image in image_list:\n",
    "        pred1 = net(image)\n",
    "        pred1 = pred1.squeeze(0).astype(np.int8)\n",
    "        break\n",
    "    prinrt(pred1)\n",
    "    return pred1\n",
    "\n",
    "pred= total_predict(Image.open(img_url))\n",
    "#print(type(pred))\n",
    "#print(pred)\n",
    "show_img(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}