{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informational-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-clone",
   "metadata": {},
   "source": [
    "## UNet with ResNet34 encoder (Pytorch)\n",
    "<code>https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch/notebook\\<code/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stunning-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (residual_function): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (residual_function): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (residual_function): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "train_mask_dir = \"../DigestPath2019/train_mask\" #create  train mask\n",
    "train_dir = \"../DigestPath2019/train\"\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    # BasicBlock and BottleNeck block\n",
    "    # have different output size\n",
    "    # we use class attribute expansion\n",
    "    # to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        # shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # the shortcut output dimension is not the same with residual function\n",
    "        # use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channel,out_channel, block, num_block):\n",
    "        super().__init__()\n",
    "        #self.in_channels = in_channel\n",
    "        self.outc = out_channel #不能加self.out_channel  不是很懂为啥 估计是变量优先级问题\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size = 7, stride = 2, padding = 3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.dconv_last=nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64,out_channel,1)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)# [stride, 1,1,1...]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        conv1 = self.conv1(x)\n",
    "        temp=self.maxpool(conv1)\n",
    "        print(temp.shape)\n",
    "        print(self.conv2_x)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        x = self.upsample(bottle)\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv2.shape)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x=self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv1.shape)\n",
    "        x=torch.cat([x,conv1],dim=1)\n",
    "        out=self.dconv_last(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "         # 导入自己模型的参数\n",
    "        model_dict=self.state_dict()\n",
    "        \n",
    "        resnet34_weights = models.resnet34(True).state_dict()\n",
    "        count_res = 0\n",
    "        count_my = 0\n",
    "\n",
    "        reskeys = list(resnet34_weights.keys())\n",
    "        mykeys = list(model_dict.keys())\n",
    "        # print(self)  自己网络的结构\n",
    "        # print(models.resnet34())   resnet结构\n",
    "        # print(reskeys)\n",
    "        # print(mykeys)\n",
    "\n",
    "        corresp_map = []\n",
    "        while (True):              # 后缀相同的放入list\n",
    "            reskey = reskeys[count_res]\n",
    "            mykey = mykeys[count_my]\n",
    "\n",
    "            if \"fc\" in reskey:\n",
    "                break\n",
    "\n",
    "            while reskey.split(\".\")[-1] not in mykey:\n",
    "                count_my += 1\n",
    "                mykey = mykeys[count_my]\n",
    "\n",
    "            corresp_map.append([reskey, mykey])\n",
    "            count_res += 1\n",
    "            count_my += 1\n",
    "\n",
    "        for k_res, k_my in corresp_map:\n",
    "            model_dict[k_my]=resnet34_weights[k_res]\n",
    "\n",
    "        try:\n",
    "            self.load_state_dict(model_dict)\n",
    "            print(\"Loaded resnet34 weights in mynet !\")\n",
    "        except:\n",
    "            print(\"Error resnet34 weights in mynet !\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet34(in_channel,out_channel,pretrain=True):\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    model=ResNet(in_channel,out_channel,BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrain:\n",
    "        model.load_pretrained_weights()\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet34(3, 4, False) # out_channel = 4  4分类问题\n",
    "    #print(net)\n",
    "    x = torch.rand((1, 3, 512, 512)) #N，C, H, W\n",
    "    print(net.forward(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-pearl",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n",
      "INFO: Network:\n",
      "\t512 input channels\n",
      "\t1 output channels (classes)\n",
      "\tBilinear upscaling\n",
      "INFO: Creating dataset with 315 examples\n",
      "INFO: Starting training:\n",
      "        Epochs:          5\n",
      "        Batch size:      1\n",
      "        Learning rate:   0.0001\n",
      "        Training size:   284\n",
      "        Validation size: 31\n",
      "        Checkpoints:     True\n",
      "        Device:          cpu\n",
      "        Images scaling:  0.5\n",
      "    \n",
      "Epoch 1/5:   0%|          | 0/284 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded resnet34 weights in mynet !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/284 [00:00<?, ?img/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 272, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-7-d233c8aa5afc>\", line 81, in __getitem__\n    f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\nAssertionError: Image and mask 2019-08261-1-1-1_2019-05-29 01_05_21-lv1-45083-13771-4787-2600 should be the same size, but are (4787, 2600) and (2600, 4787)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d233c8aa5afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    251\u001b[0m                   \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                   \u001b[0mimg_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                   val_percent=args.val / 100)\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INTERRUPTED.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d233c8aa5afc>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, epochs, batch_size, lr, val_percent, save_cp, img_scale)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mtrue_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 272, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-7-d233c8aa5afc>\", line 81, in __getitem__\n    f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\nAssertionError: Image and mask 2019-08261-1-1-1_2019-05-29 01_05_21-lv1-45083-13771-4787-2600 should be the same size, but are (4787, 2600) and (2600, 4787)\n"
     ]
    }
   ],
   "source": [
    "# version 1 mile\n",
    "# refer : https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "'''dir_img = 'data/imgs/'\n",
    "dir_mask = 'data/masks/'\n",
    "dir_checkpoint = 'checkpoints/'\n",
    "\n",
    "'''\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, scale=1, mask_suffix='_mask'):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix    \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]   # get prefix or so-called id\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "        #print(\"image_directory:{}  with {} files.\\nmask_dir:{}\".format(imgs_dir,len(self.ids),masks_dir))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    "\n",
    "        img_nd = np.array(pil_img)\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW   \n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans\n",
    "       \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        temp = os.path.join(self.masks_dir,idx+self.mask_suffix+'.*')\n",
    "        # print(type(temp))\n",
    "        #print(\"i:\"+temp+\"\\n\")\n",
    "        mask_file = glob.glob(temp)\n",
    "        \n",
    "        img_file = glob.glob(os.path.join(self.imgs_dir , idx + '.*'))\n",
    "        \n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(img, self.scale)\n",
    "        mask = self.preprocess(mask, self.scale)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }\n",
    "\n",
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=10,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              img_scale=0.5):\n",
    "\n",
    "    dataset = BasicDataset(train_dir, train_mask_dir, img_scale)\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train, val = random_split(dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n",
    "    global_step = 0\n",
    "    # 基本信息\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "    ''')\n",
    "    #优化器\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.outc > 1 else 'max', patience=2)\n",
    "    if net.outc > 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if net.outc == 1 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                if global_step % (n_train // (10 * batch_size)) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=5,\n",
    "                        help='Number of epochs', dest='epochs')\n",
    "    parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=1,\n",
    "                        help='Batch size', dest='batchsize')\n",
    "    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.0001,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('-f', '--load', dest='load', type=str, default=False,\n",
    "                        help='Load model from a .pth file')\n",
    "    parser.add_argument('-s', '--scale', dest='scale', type=float, default=0.5,\n",
    "                        help='Downscaling factor of the images')\n",
    "    parser.add_argument('-v', '--validation', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    args = get_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Change here to adapt to your data\n",
    "    # n_channels=3 for RGB images\n",
    "    # n_classes is the number of probabilities you want to get per pixel\n",
    "    #   - For 1 class and background, use n_classes=1\n",
    "    #   - For 2 classes, use n_classes=1\n",
    "    #   - For N > 2 classes, use n_classes=N\n",
    "    #net = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "    net = resnet34(in_channel=3, out_channel=1)\n",
    "\n",
    "    logging.info(f'Network:\\n'\n",
    "                 f'\\t{net.in_channels} input channels\\n'\n",
    "                 f'\\t{net.outc} output channels (classes)\\n'\n",
    "                 f'\\t{\"Bilinear\" if args.load else \"Transposed conv\"} upscaling')\n",
    "    \n",
    "    '''？？？\n",
    "        if args.load:\n",
    "            net.load_state_dict(\n",
    "                torch.load(args.load, map_location=device)\n",
    "            )\n",
    "            logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "    '''\n",
    "\n",
    "    net.to(device=device)\n",
    "    # faster convolutions, but more memory\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batchsize,\n",
    "                  lr=args.lr,\n",
    "                  device=device,\n",
    "                  img_scale=args.scale,\n",
    "                  val_percent=args.val / 100)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        logging.info('Saved interrupt')\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "expired-corner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-karen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
