{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet with ResNet34 encoder (Pytorch)\n",
    "<code>https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch/notebook\\<code/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import random\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "train_mask_dir = \"../DigestPath2019/train_mask\" #create  train mask\n",
    "train_dir = \"../DigestPath2019/train\"\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        #nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    # BasicBlock and BottleNeck block\n",
    "    # have different output size\n",
    "    # we use class attribute expansion\n",
    "    # to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        # shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # the shortcut output dimension is not the same with residual function\n",
    "        # use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channel,out_channel, block, num_block):\n",
    "        super().__init__()\n",
    "        #self.in_channels = in_channel\n",
    "        self.outc = out_channel #不能加self.out_channel  不是很懂为啥 估计是变量优先级问题\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, kernel_size = 7, stride = 2, padding = 3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # we use a different inputsize than the original paper\n",
    "        # so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.dconv_last=nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64,out_channel,1)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)# [stride, 1,1,1...]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        conv1 = self.conv1(x)\n",
    "        temp=self.maxpool(conv1)\n",
    "        print(temp.shape)\n",
    "        print(self.conv2_x)\n",
    "        conv2 = self.conv2_x(temp)\n",
    "        conv3 = self.conv3_x(conv2)\n",
    "        conv4 = self.conv4_x(conv3)\n",
    "        bottle = self.conv5_x(conv4)\n",
    "        # output = self.avg_pool(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        # output = self.fc(output)\n",
    "        x = self.upsample(bottle)\n",
    "        # print(x.shape)\n",
    "        # print(conv4.shape)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv3.shape)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv2.shape)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x=self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        # print(conv1.shape)\n",
    "        x=torch.cat([x,conv1],dim=1)\n",
    "        out=self.dconv_last(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "         # 导入自己模型的参数\n",
    "        model_dict=self.state_dict()\n",
    "        \n",
    "        resnet34_weights = models.resnet34(True).state_dict()\n",
    "        count_res = 0\n",
    "        count_my = 0\n",
    "\n",
    "        reskeys = list(resnet34_weights.keys())\n",
    "        mykeys = list(model_dict.keys())\n",
    "        # print(self)  自己网络的结构\n",
    "        # print(models.resnet34())   resnet结构\n",
    "        # print(reskeys)\n",
    "        # print(mykeys)\n",
    "\n",
    "        corresp_map = []\n",
    "        while (True):              # 后缀相同的放入list\n",
    "            reskey = reskeys[count_res]\n",
    "            mykey = mykeys[count_my]\n",
    "\n",
    "            if \"fc\" in reskey:\n",
    "                break\n",
    "\n",
    "            while reskey.split(\".\")[-1] not in mykey:\n",
    "                count_my += 1\n",
    "                mykey = mykeys[count_my]\n",
    "\n",
    "            corresp_map.append([reskey, mykey])\n",
    "            count_res += 1\n",
    "            count_my += 1\n",
    "\n",
    "        for k_res, k_my in corresp_map:\n",
    "            model_dict[k_my]=resnet34_weights[k_res]\n",
    "\n",
    "        try:\n",
    "            self.load_state_dict(model_dict)\n",
    "            print(\"Loaded resnet34 weights in mynet !\")\n",
    "        except:\n",
    "            print(\"Error resnet34 weights in mynet !\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet34(in_channel,out_channel,pretrain=True):\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    model=ResNet(in_channel,out_channel,BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrain:\n",
    "        model.load_pretrained_weights()\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet34(3, 4, False) # out_channel = 4  4分类问题\n",
    "    #print(net)\n",
    "    x = torch.rand((1, 3, 512, 512)) #N，C, H, W\n",
    "    print(net.forward(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, preprocess=None,scale=0.1, mask_suffix='_mask'):      \n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.preprocess= preprocess\n",
    "        #self.scale = scale\n",
    "        self.scale = 512\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]   # get prefix or so-called id\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "        #print(\"image_directory:{}  with {} files.\\nmask_dir:{}\".format(imgs_dir,len(self.ids),masks_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def process(cls, pil_img):\n",
    "        #w, h = pil_img.size\n",
    "        img = np.array(pil_img)\n",
    "        # unnormalize\n",
    "        opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        #print(\"cv2 show by process\")\n",
    "        #cv2.imshow(\"sss\",opencvImage)\n",
    "        \n",
    "        return opencvImage\n",
    "        #newW, newH = int(scale * w), int(scale * h)\n",
    "        #assert w > 0 and h > 0, 'Scale is too small\n",
    "        #pil_img = pil_img.resize((size, size))\n",
    "\n",
    "        ''' img_nd = np.array(pil_img)\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW\n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans'''\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        temp = os.path.join(self.masks_dir,idx+self.mask_suffix+'.*')\n",
    "        mask_file = glob.glob(temp)\n",
    "\n",
    "        img_file = glob.glob(os.path.join(self.imgs_dir , idx + '.*'))\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        #print(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "        img = self.process(img)\n",
    "        mask = self.process(mask)\n",
    "        if self.preprocess!= None:\n",
    "            transformed = self.preprocess(image=img,mask=mask)\n",
    "            img= transformed['image']\n",
    "            mask= transformed['mask']\n",
    "        img_trans = img.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "        #print(mask.shape)\n",
    "        Grayimg = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask_trans = cv2.threshold(Grayimg, 12, 255,cv2.THRESH_BINARY)\n",
    "\n",
    "        \n",
    "        _ = {\n",
    "            'image': torch.from_numpy(img_trans).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask_trans).type(torch.FloatTensor).unsqueeze(0)\n",
    "        }      \n",
    "        print(\"image final .shape\",_['image'].shape)\n",
    "\n",
    "        print(\"mask final .shape\",_['mask'].shape)\n",
    "        #show_img(img)\n",
    "        return _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n",
    "    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n",
    "    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n",
    "    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n",
    ")\n",
    "size = 512\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    OneOf([\n",
    "        RandomContrast(),\n",
    "        RandomGamma(),\n",
    "        RandomBrightness()\n",
    "    ], p=0.3),\n",
    "    OneOf([\n",
    "        ElasticTransform(alpha = 120, sigma=120*0.05,alpha_affine = 12*0.03),\n",
    "        GridDistortion(),\n",
    "        OpticalDistortion(distort_limit = 2, shift_limit = 0.5)\n",
    "    ], p=0.3),\n",
    "    RandomSizedCrop(min_max_height=(512,1024),height = size, width =size,p=1),\n",
    "    ToFloat(max_value=1)\n",
    "], p =1)\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    ToFloat(max_value=1)\n",
    "],p=1)\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    # unnormalize\n",
    "    #npimg=Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.imshow(img.astype(int))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_url=\"../Colonoscopy_tissue_segment_dataset/train/1902160001_2019-06-11 12_36_32-lv1-39045-16016-3312-4096.jpg\"\n",
    "image = Image.open(img_url)\n",
    "img = np.array(image)\n",
    "\n",
    "print(img.size)\n",
    "\n",
    "print(image.size)\n",
    "\n",
    "opencvImage = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# unnormalize\n",
    "print(\"cv2 show by process\")\n",
    "#cv2.imshow(\"sss\",opencvImage)\n",
    "opencvImage=AUGMENTATIONS_TRAIN(image=opencvImage)['image']\n",
    "print(opencvImage.shape)\n",
    "img = opencvImage.transpose((2, 0, 1))\n",
    "if img.max() > 1:\n",
    "    img = img / 255 \n",
    "print(img.shape)\n",
    "\n",
    "tmp = torch.from_numpy(img).type(torch.FloatTensor)\n",
    "\n",
    "print(\"tmp\",tmp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-06bcdf06bc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m net.load_state_dict(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth'"
     ]
    }
   ],
   "source": [
    "import predict \n",
    "from train import BasicDataset\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "from augment import AUGMENTATIONS_TEST2 as AUGMENTATIONS_TEST\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import cv2\n",
    "device = \"cpu\"\n",
    "img_url = \"../Colonoscopy_tissue_segment_dataset/val/18_00991B_2019-05-07 21_27_54-lv1-16174-30030-3538-5736.jpg\"\n",
    "\n",
    "load=\"/Users/eggwardhan/Documents/cv/Segmentation-of-colon-tumor-cells-based-on-pathological-images/check_point/nest_unet.pth\"\n",
    "net = model.choose_net(\"nested_unet\")\n",
    "\n",
    "net = net(in_channel=3,out_channel=1)\n",
    "net.load_state_dict(\n",
    "            torch.load(load, map_location=device)\n",
    "            )\n",
    "\n",
    "def total_predict(ori_image):\n",
    "    # PIL Get image size (width, height) \n",
    "    # nparray get image size (row (height), column (width), color (3))\n",
    "    img =Image.open(img_url)\n",
    "    #print(img.size)\n",
    "    ori_image=np.array(img)\n",
    "    #ori_image = process(ori_image)\n",
    "    #print(\"ori_image:\",ori_image.shape)\n",
    "    h_step = ori_image.shape[0]//256\n",
    "    w_step = ori_image.shape[1]//256\n",
    "\n",
    "    h_rest = -(ori_image.shape[0] - 256 * h_step)\n",
    "    w_rest = -(ori_image.shape[1] - 256 * w_step)\n",
    "    image_list = []\n",
    "    predict_list = []\n",
    "    for h in range(h_step):      # 截取片段\n",
    "        for w in range(w_step):\n",
    "            image_sample = ori_image[ (h*256):(h*256+256),\n",
    "            (w*256 ) : (w*256 + 256), : ]/255\n",
    "            image_list.append(image_sample)  \n",
    "        image_list.append(ori_image[( h* 256) : (h*256 +256), -256:, :]/255)\n",
    "    for w in range(w_step-1):   \n",
    "        image_list.append(ori_image[-256:, (w*256):(w*256 +256), :]/255)\n",
    "    image_list.append(ori_image[-256:, -256:, :]/255)\n",
    "\n",
    "    for image in image_list:       \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        image = image.unsqueeze(0)\n",
    "        #print(image.shape)\n",
    "        image.to(device=\"cuda\", dtype=torch.float32)\n",
    "        pred1 = net(image)\n",
    "        pred1 = pred1.squeeze(0).squeeze(0)  # 2 dimension\n",
    "        pred = pred1>threshold\n",
    "        predict_list.append(pred)\n",
    "    count_temp = 0\n",
    "    tmp = np.ones([ori_image.shape[0],ori_image.shape[1]])\n",
    "    for h in range(h_step):\n",
    "        for w in range(w_step):\n",
    "            tmp[\n",
    "                h*256:(h+1)*256,\n",
    "                w*256:(w+1)*256\n",
    "            ] = predict_list[count_temp]\n",
    "            count_temp += 1\n",
    "        tmp[h *256 :(h+1) *256, w_rest:] = predict_list[count_temp][:, w_rest:]\n",
    "        count_temp+=1\n",
    "    for w in range(w_step -1):\n",
    "        tmp[h_rest:, (w *256):(w*256+256)] = predict_list[count_temp][h_rest:,:]\n",
    "        count_temp+=1\n",
    "    tmp[-257:-1,-257:-1] = predict_list[count_temp][:, :]\n",
    "\n",
    "pred= total_predict(Image.open(img_url))\n",
    "#print(type(pred))\n",
    "#print(pred)\n",
    "show_img(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper module that consists of a Conv -> BN -> ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.with_nonlinearity = with_nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.with_nonlinearity:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bridge(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the middle layer of the UNet which just consists of some\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bridge = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bridge(x)\n",
    "\n",
    "\n",
    "class UpBlockForUNetWithResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
    "                 upsampling_method=\"conv_transpose\"):\n",
    "        super().__init__()\n",
    "        self.n_channels=in_channels\n",
    "        self.outc=self.n_classes=out_channels\n",
    "\n",
    "\n",
    "        if up_conv_in_channels == None:\n",
    "            up_conv_in_channels = in_channels\n",
    "        if up_conv_out_channels == None:\n",
    "            up_conv_out_channels = out_channels\n",
    "\n",
    "        if upsampling_method == \"conv_transpose\":\n",
    "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
    "        elif upsampling_method == \"bilinear\":\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
    "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, up_x, down_x):\n",
    "        \"\"\"\n",
    "        :param up_x: this is the output from the previous up block\n",
    "        :param down_x: this is the output from the down block\n",
    "        :return: upsampled feature map\n",
    "        \"\"\"\n",
    "        x = self.upsample(up_x)\n",
    "        x = torch.cat([x, down_x], 1)\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetWithResnet50Encoder(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self,in_channel, out_channel=1):\n",
    "        super().__init__()\n",
    "        self.n_channels = in_channel\n",
    "        self.outc=self.n_classes=out_channel\n",
    "\n",
    "        resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "        self.input_pool = list(resnet.children())[3]\n",
    "        for bottleneck in list(resnet.children()):\n",
    "            if isinstance(bottleneck, nn.Sequential):\n",
    "                down_blocks.append(bottleneck)\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, out_channel, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return x\n",
    "        print(x)\n",
    "#model = UNetWithResnet50Encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.UNetWithResnet50Encoder'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Network:\n",
      "\t3 input channels\n",
      "\t1 output channels (classes)\n",
      "\tBilinear upscaling\n",
      "INFO: Creating dataset with 297 examples\n",
      "INFO: Creating dataset with 33 examples\n",
      "INFO: Starting training:\n",
      "        Epochs:          2\n",
      "        Batch size:      4\n",
      "        Criterion:       dice\n",
      "        Learning rate:   0.001\n",
      "        Training size:   297\n",
      "        Validation size: 33\n",
      "        Checkpoints:     True\n",
      "        Device:          cpu\n",
      "        Images scaling:  256\n",
      "    \n",
      "Epoch 1/2:   0%|          | 0/297 [00:00<?, ?img/s]"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# version 1 mile\n",
    "# refer : https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import cv2\n",
    "import model.model as model\n",
    "from eval import eval_net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from augment import AUGMENTATIONS_TRAIN,AUGMENTATIONS_TEST\n",
    "Image.MAX_IMAGE_PIXELS=None\n",
    "\n",
    "'''\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils.dataset import BasicDataset\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "dir_checkpoint= \"./check_point/\"\n",
    "\n",
    "train_mask_dir = \"../Colonoscopy_tissue_segment_dataset/train_mask\" #create  train mask\n",
    "train_dir = \"../Colonoscopy_tissue_segment_dataset/train\" # create train data\n",
    "val_mask_dir = \"../Colonoscopy_tissue_segment_dataset/val_mask\"\n",
    "val_dir = \"../Colonoscopy_tissue_segment_dataset/val\"\n",
    "\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, preprocess=None, scale=512, mask_suffix='_mask'):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        #self.scale = scale\n",
    "        self.preprocess= preprocess\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]   # get prefix or so-called id\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "        #print(\"image_directory:{}  with {} files.\\nmask_dir:{}\".format(imgs_dir,len(self.ids),masks_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def process(cls, pil_img,pil_mask,preprocess=None):\n",
    "        img = np.array(pil_img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        mask = np.array(pil_mask)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)\n",
    "        if preprocess!= None:\n",
    "            transformed = preprocess(image=img,mask=mask)\n",
    "            img= transformed['image']\n",
    "            mask= transformed['mask']\n",
    "\n",
    "        img_trans = img.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "        #mask_trans = mask.transpose((2,0,1))\n",
    "        #print(mask.shape)\n",
    "        Grayimg = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask_trans = cv2.threshold(Grayimg, 12, 255,cv2.THRESH_BINARY)\n",
    "        #cv2.imshow(\"sss\",opencvImage)\n",
    "        _ = {\n",
    "            'image': torch.from_numpy(img_trans).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask_trans).type(torch.FloatTensor).unsqueeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "        return _\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        temp = os.path.join(self.masks_dir,idx+self.mask_suffix+'.*')\n",
    "        mask_file = glob.glob(temp)\n",
    "\n",
    "        img_file = glob.glob(os.path.join(self.imgs_dir , idx + '.*'))\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "        #show_img(img)\n",
    "        #img = self.process(img)\n",
    "        #mask = self.process(mask)\n",
    "\n",
    "        '''\n",
    "        mask_trans = mask.transpose((2, 0, 1))\n",
    "        if mask_trans.max() > 1:\n",
    "            mask_trans = mask_trans / 255\n",
    "        '''\n",
    "        _ =  self.process(img,mask, self.preprocess)\n",
    "        #print(\"mask shape\",_['mask'].shape)\n",
    "       # print(\"mask tensor\",_['mask'])\n",
    "        #print(\"image shape\",_['image'].shape)\n",
    "        return _\n",
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=10,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              criterion=\"dice\",\n",
    "              img_scale=512):\n",
    "\n",
    "    #dataset = BasicDataset(train_dir, train_mask_dir, img_scale)\n",
    "    dataset = BasicDataset(train_dir, train_mask_dir,AUGMENTATIONS_TRAIN,img_scale)\n",
    "    dataset2 = BasicDataset(val_dir,val_mask_dir,AUGMENTATIONS_TEST)\n",
    "    n_train=len(dataset)\n",
    "    n_val = len(dataset2)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'{args.net}_{lr}_{batch_size}_{img_scale}_{criterion}')\n",
    "    global_step = 0\n",
    "    # 基本信息\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Criterion:       {criterion}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {len(dataset)}\n",
    "        Validation size: {len(dataset2)}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "    ''')\n",
    "    #优化器\n",
    "    #optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-4, momentum=0.9)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9,weight_decay=1e-4)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr,  weight_decay=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.outc > 1 else 'max',factor=0.5, patience=8)\n",
    "    #if net.outc > 1:\n",
    "    def dice_loss(input , target):\n",
    "        input = torch.sigmoid(input)\n",
    "        smooth = 0.00001\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        return 1-((2.0*intersection + smooth)/(iflat.sum()+tflat.sum()+smooth))\n",
    "    def combo_loss(input,target,alpha=0.6):\n",
    "        loss1=nn.BCEWithLogitsLoss(torch.Tensor([7]).to(device))\n",
    "        tmp = alpha*loss1(input,target)-(1-alpha)*(1-dice_loss(input,target))\n",
    "        return tmp\n",
    "    def generalized_loss(input,target):\n",
    "        dice_l = dice_loss(input,target)\n",
    "        input = torch.sigmoid(input)\n",
    "\n",
    "\n",
    "    if  criterion==\"bce\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif criterion ==  \"cross\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif criterion ==\"combo\":\n",
    "        criterion = combo_loss\n",
    "    else:\n",
    "        criterion = dice_loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        best_auc=0\n",
    "        best_dice=0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']/255\n",
    "                #true_masks =batch['mask']\n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                #mask_type = torch.float32 if net.outc == 1 else torch.long\n",
    "                mask_type = torch.float32\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "                #print(\"true mask shape: %s \" % true_masks.shape)\n",
    "                masks_pred = net(imgs)\n",
    "                #print(\"predict mask shape:%s\" % mask_pred.shape)\n",
    "                if net.deep_supervision==True:\n",
    "                    loss_s=0\n",
    "                    for output in masks_pred:\n",
    "                        loss_s+=criterion(output,true_masks)\n",
    "                    loss=loss_s/len(output)\n",
    "                else:\n",
    "                    loss = criterion(masks_pred,true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                if global_step % (n_train // (10 * batch_size)) == 0:\n",
    "                    logging.info('item_loss: {}'.format(epoch_loss))\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score[\"loss\"])\n",
    "                    if best_auc<val_score[\"auc\"] and best_dice<val_score[\"loss\"]:\n",
    "                        best_auc=val_score[\"auc\"]\n",
    "                        best_dice=val_score[\"loss\"]\n",
    "\n",
    "\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                    logging.info('lr: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score[\"loss\"]))\n",
    "                        writer.add_scalar('Loss/test', val_score[\"loss\"], global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score[\"loss\"]))\n",
    "                        writer.add_scalar('Dice/test', val_score[\"loss\"], global_step)\n",
    "                        logging.info(\"Auc coeff:{}\".format(val_score[\"auc\"]))\n",
    "                        writer.add_scalar(\"Auc/test\",val_score[\"auc\"],global_step)\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        if net.deep_supervision==True:\n",
    "                            writer.add_images('masks/pred', torch.sigmoid(masks_pred[-1]) > 0.5, global_step)\n",
    "                        else:\n",
    "                            writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       dir_checkpoint + f'{args.net}batch{str(args.batchsize)}scale{args.scale}criterion{args.criterion}epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !best_auc:{best_auc},best_dice:{best_dice}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=5,\n",
    "                        help='Number of epochs', dest='epochs')\n",
    "    parser.add_argument('-n', '--net' ,dest=\"net\",type=str,default=\"unet\",\n",
    "                        help=\"choose net like resnet\")\n",
    "    parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=4,\n",
    "                        help='Batch size', dest='batchsize')\n",
    "    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.0001,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('-f', '--load', dest='load', type=str, default=False,\n",
    "                        help='Load model from a .pth file')\n",
    "    parser.add_argument('-s', '--scale', dest='scale', type=float, default=256,\n",
    "                        help='Downscaling factor of the images')\n",
    "    parser.add_argument('-v', '--validation', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('-c', '--criterion', dest='criterion',type=str,default='dice',\n",
    "                        help = \"criterion like bce crossentropyloss, dice_loss\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    args = get_args()\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device =torch.device(\"cuda:1\")\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Change here to adapt to your data\n",
    "    # n_channels=3 for RGB images\n",
    "    # n_classes is the number of probabilities you want to get per pixel\n",
    "    #   - For 1 class and background, use n_classes=1\n",
    "    #   - For 2 classes, use n_classes=1\n",
    "    #   - For N > 2 classes, use n_classes=N\n",
    "    net =  UNetWithResnet50Encoder\n",
    "    print(net)\n",
    "    net = net(3,1)\n",
    "    logging.info(f'Network:\\n'\n",
    "                f'\\t{net.n_channels} input channels\\n'\n",
    "                 f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "                 f'\\t{\"Bilinear\" if args.load else \"Transposed conv\"} upscaling')\n",
    "\n",
    "    \n",
    "\n",
    "    args.batchsize=4\n",
    "    args.lr=0.001\n",
    "    args.device=\"cpu\"\n",
    "    net.to(device=device)\n",
    "    args.epochs=2\n",
    "    # faster convolutions, but more memory\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batchsize,\n",
    "                  lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  device=device,\n",
    "                  img_scale=args.scale,\n",
    "                  val_percent=args.val / 100)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), \"./interupted/\"+args.net+str(args.batchsize)+args.criterion+'.pth')\n",
    "        logging.info('Saved interrupt')\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsh_env",
   "language": "python",
   "name": "hsh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
